{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "000906eb-5032-4d45-b19a-7844b8cb6e5d",
   "metadata": {},
   "source": [
    "# Advanced parking lot detection system\n",
    "Down below, we're gonna teach a Support Vector Machine to recognize when a parking spot is empty.\n",
    "Our clever plan involves using a croner or an edge detection algorithm to find corners and edges in different training examples, summing them.<br>\n",
    "The idea is that if there's a car present, there will be more detections, resulting in a higher sum value.\n",
    "In the .csv file, we'll use the occupancy field as our label. As for the parking spots themselves, they're represented by three features: the sum of pixel values after applying the algorithm, plus the weather and the hour when the image was taken.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f83e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ee24dd-280d-4522-889b-8532b4a70681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset\n",
    "path_to_data = \"CNRPark+EXT.csv\"\n",
    "data = pd.read_csv(path_to_data, low_memory=False)\n",
    "\n",
    "# Load image urls\n",
    "split = 12584\n",
    "image_url = data[\"image_url\"].to_numpy()\n",
    "\n",
    "# Create the trainset\n",
    "trainset = np.empty((data.shape[0],3), dtype=\"object\")\n",
    "\n",
    "# Feature number zero is the weather\n",
    "def map_weather(w):\n",
    "    return 0 if w == \"S\" else (1 if w == \"R\" else 2)\n",
    "\n",
    "weather = data[\"weather\"].to_numpy()\n",
    "weather_mapper = np.vectorize(map_weather)\n",
    "trainset[:,0] = weather_mapper(weather)\n",
    "\n",
    "# Feature number one is the hour\n",
    "trainset[:,1] = data[\"hour\"].to_numpy()\n",
    "label = data[\"occupancy\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f1fa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_from_url(image):\n",
    "    return image if isinstance(image, np.ndarray) else cv2.imread(image, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "\n",
    "# Define a function for each different Edge and Corner Detector\n",
    "def canny_edge_count(image_url):\n",
    "    image = get_img_from_url(image_url)\n",
    "    # image = cv2.equalizeHist(image)\n",
    "    \n",
    "    lt, ht, size = 50, 200, 5\n",
    "    edges = cv2.Canny(image, lt, ht, size)\n",
    "    \n",
    "    return np.sum(edges) / (edges.shape[0] * edges.shape[1])\n",
    "\n",
    "def harris_corner_count(image_url):\n",
    "    # Performing a GaussianBlur to remove noises and small structures\n",
    "    image = get_img_from_url(image_url)\n",
    "    blurred_image = cv2.GaussianBlur(image, (15, 15), 0)\n",
    "    \n",
    "    corners = cv2.cornerHarris(blurred_image, 2, 3, 0.04)\n",
    "    \n",
    "    return corners.sum()\n",
    "\n",
    "def susan_corner_count(image_url):\n",
    "    # Performing a GaussianBlur to remove noises and small structures\n",
    "    image = get_img_from_url(image_url)\n",
    "    blurred_image = cv2.GaussianBlur(image, (15, 15), 0)\n",
    "\n",
    "    susan = cv2.ximgproc.createFastFeatureDetector()\n",
    "    keypoints = susan.detect(blurred_image)\n",
    "    \n",
    "    return len(keypoints)\n",
    "\n",
    "def fast_corner_count(image_url):\n",
    "    # Performing a GaussianBlur to remove noises and small structures\n",
    "    image = get_img_from_url(image_url)\n",
    "    blurred_image = cv2.GaussianBlur(image, (15, 15), 0)\n",
    "    \n",
    "    fast = cv2.FastFeatureDetector_create()\n",
    "    keypoints = fast.detect(blurred_image, None)\n",
    "    \n",
    "    return len(keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308adbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the canny edge count on the disk for reuse\n",
    "canny_mapper = np.vectorize(canny_edge_count)\n",
    "canny_edges = canny_mapper(image_url)\n",
    "\n",
    "# Saving the harris corner count on the disk for reuse\n",
    "harris_mapper = np.vectorize(harris_corner_count)\n",
    "harris_corners = harris_mapper(image_url)\n",
    "\n",
    "# Saving the susan corner count on the disk for reuse\n",
    "susan_mapper = np.vectorize(susan_corner_count)\n",
    "susan_corners = None\n",
    "\n",
    "# Saving the fast corner count on the disk for reuse\n",
    "fast_mapper = np.vectorize(harris_corner_count)\n",
    "fast_corners = fast_mapper(image_url)\n",
    "\n",
    "with open(\"edge_corner_count.pkl\", 'wb') as file:  \n",
    "    pickle.dump([canny_edges, harris_corners, susan_corners, fast_corners], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edf145e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"edge_corner_count.pkl\", 'rb') as file:  \n",
    "    canny_edges, harris_corners, susan_corners, fast_corners = pickle.load(file)\n",
    "\n",
    "# Feature number two is the edge or corner count\n",
    "trainset[:,2] = harris_corners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285ddba7-f7bf-41d1-8b14-f690b685efc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for the fit\n",
    "X = trainset    # Trainset is the X\n",
    "y = label       # Labels are the y\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train the SVM model\n",
    "model = svm.SVC()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = model.score(X_test_scaled, y_test)\n",
    "print(\"Accuracy:\", accuracy * 100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960f9dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model on the disk\n",
    "with open(\"svm_harris_model.pkl\", 'wb') as file:  \n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be2657e-725c-4b48-a8e9-ba8556e50007",
   "metadata": {},
   "source": [
    "# Fullsize image testing\n",
    "Algorithm testing with high qquality parking images provided by the CNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f672d8-50fc-44e7-8df6-c34255187d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "\n",
    "# Select a random full image\n",
    "random_index = int(split + (split - data.shape[0]) * np.random.rand())\n",
    "img_descriptor = data.iloc[random_index]\n",
    "\n",
    "# Compute the path of the fullsize image\n",
    "resized_path = \"CNR-EXT/PATCHES\"\n",
    "fullsize_path = \"CNR-EXT_FULL_IMAGE_1000x750/FULL_IMAGE_1000x750\"\n",
    "\n",
    "filename = (\"%d-%02d-%02d_%02d%02d.jpg\" % (img_descriptor[\"year\"], img_descriptor[\"month\"], img_descriptor[\"day\"], \n",
    "                                           img_descriptor[\"hour\"], img_descriptor[\"minute\"]))\n",
    "\n",
    "\n",
    "image_path = img_descriptor[\"image_url\"].replace(resized_path, fullsize_path)\n",
    "image_path = path.dirname(image_path)\n",
    "image_path = path.join(image_path, filename)\n",
    "\n",
    "# Import and resize the image\n",
    "img = cv2.imread(image_path)\n",
    "img = cv2.resize(img, (2592,1944), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "# Convert to grayscale to perform the algorithm on it\n",
    "gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Import the csv file corrisponding to the right camera\n",
    "camera_csv = (\"CNR-EXT_FULL_IMAGE_1000x750/camera%01d.csv\" % int(img_descriptor[\"camera\"]))\n",
    "camera_descriptor = pd.read_csv(camera_csv)\n",
    "\n",
    "# Create a variable containing data to be predicted\n",
    "slot = np.empty((camera_descriptor.shape[0],3), dtype=\"object\")\n",
    "\n",
    "# Map the weather\n",
    "slot[:,0] = map_weather(img_descriptor[\"weather\"])\n",
    "\n",
    "# Insert the hour\n",
    "slot[:,1] = img_descriptor[\"hour\"]\n",
    "\n",
    "# Get slots informations\n",
    "x, y, w, h = camera_descriptor.values[:,1], camera_descriptor.values[:,2], camera_descriptor.values[:,3], camera_descriptor.values[:,4]\n",
    "\n",
    "# Perform the edge or corner detection on those boundaries\n",
    "for i in range(camera_descriptor.shape[0]):\n",
    "    slot_img = gray_img[y[i] : y[i] + h[i], x[i] : x[i] + w[i]]\n",
    "    slot[i,2] = harris_corner_count(slot_img)\n",
    "\n",
    "# Perform the prediction\n",
    "slot = scaler.transform(slot)\n",
    "result = model.predict(slot)\n",
    "\n",
    "# Draw rectangles in park slots\n",
    "for i in range(len(result)):\n",
    "    if result[i]==0:\n",
    "        colore=(0,255,0)\n",
    "    else:\n",
    "        colore=(0,0,255)\n",
    "    cv2.rectangle(img, (x[i], y[i]), (x[i] + w[i], y[i] + h[i]), colore, 2)\n",
    "\n",
    "# Display the image\n",
    "img = cv2.resize(img, (1000,750), interpolation=cv2.INTER_LINEAR)\n",
    "cv2.imshow(\"Parcheggi\", img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d13617c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
